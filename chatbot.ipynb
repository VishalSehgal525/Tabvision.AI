{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the model libraries\n",
    "from transformers import (DetrImageProcessor, TableTransformerForObjectDetection)\n",
    "from vietocr.tool.config import Cfg\n",
    "from vietocr.tool.predictor import Predictor\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalsehgal/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/vishalsehgal/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight /var/folders/d7/hprm3gx57ks7mrj3gv4x0q700000gn/T/vgg_seq2seq.pth exsits. Ignore download!\n"
     ]
    }
   ],
   "source": [
    "# Loading all Models\n",
    "table_detection_model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\")\n",
    "table_recognition_model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-structure-recognition\")\n",
    "config = Cfg.load_config_from_name('vgg_seq2seq')\n",
    "config['cnn']['pretrained'] = False\n",
    "config['device'] = 'cpu'\n",
    "config['predictor']['beamsearch'] = False\n",
    "detector = Predictor(config)\n",
    "pipe = pipeline(\"table-question-answering\", model=\"google/tapas-base-finetuned-wtq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from itertools import count, tee\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from tabula import read_pdf\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "def PIL_to_cv(pil_img):\n",
    "    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def cv_to_PIL(cv_img):\n",
    "    return Image.fromarray(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def pytess(cell_pil_img):\n",
    "    text, prob = detector.predict(cell_pil_img, return_prob=True)\n",
    "    if prob < 0.5:\n",
    "        return \"\"\n",
    "    return text.strip()\n",
    "\n",
    "def sharpen_image(pil_img):\n",
    "    img = PIL_to_cv(pil_img)\n",
    "    sharpen_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    sharpen = cv2.filter2D(img, -1, sharpen_kernel)\n",
    "    pil_img = cv_to_PIL(sharpen)\n",
    "    return pil_img\n",
    "\n",
    "def uniquify(seq, suffs=count(1)):\n",
    "    not_unique = [k for k, v in Counter(seq).items() if v > 1]\n",
    "    suff_gens = dict(zip(not_unique, tee(suffs, len(not_unique))))\n",
    "    for idx, s in enumerate(seq):\n",
    "        try:\n",
    "            suffix = str(next(suff_gens[s]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        else:\n",
    "            seq[idx] += suffix\n",
    "    return seq\n",
    "\n",
    "def binarizeBlur_image(pil_img):\n",
    "    image = PIL_to_cv(pil_img)\n",
    "    thresh = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "    result = cv2.GaussianBlur(thresh, (5, 5), 0)\n",
    "    result = 255 - result\n",
    "    return cv_to_PIL(result)\n",
    "\n",
    "def td_postprocess(pil_img):\n",
    "    # Removes gray background from tables\n",
    "    img = PIL_to_cv(pil_img)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(hsv, (0, 0, 100), (255, 5, 255))  # (0, 0, 100), (255, 5, 255)\n",
    "    nzmask = cv2.inRange(hsv, (0, 0, 5), (255, 255, 255))  # (0, 0, 5), (255, 255, 255))\n",
    "    nzmask = cv2.erode(nzmask, np.ones((3, 3)))  # (3,3)\n",
    "    mask = mask & nzmask\n",
    "    new_img = img.copy()\n",
    "    new_img[np.where(mask)] = 255\n",
    "    return cv_to_PIL(new_img)\n",
    "\n",
    "def table_detector(image):\n",
    "    # Table detection using DEtect-object TRansformer pre-trained on 1 million tables\n",
    "    THRESHOLD_PROBA = 0.6\n",
    "    feature_extractor = DetrImageProcessor(do_resize=True, size=800, max_size=800)\n",
    "    encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = table_detection_model(**encoding)\n",
    "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > THRESHOLD_PROBA\n",
    "    target_sizes = torch.tensor(image.size[::-1]).unsqueeze(0)\n",
    "    postprocessed_outputs = feature_extractor.post_process(outputs, target_sizes)\n",
    "    bboxes_scaled = postprocessed_outputs[0]['boxes'][keep]\n",
    "    return (probas[keep], bboxes_scaled)\n",
    "\n",
    "def table_struct_recog(image):\n",
    "    THRESHOLD_PROBA = 0.8\n",
    "    feature_extractor = DetrImageProcessor(do_resize=True, size=1000, max_size=1000)\n",
    "    encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = table_recognition_model(**encoding)\n",
    "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > THRESHOLD_PROBA\n",
    "    target_sizes = torch.tensor(image.size[::-1]).unsqueeze(0)\n",
    "    postprocessed_outputs = feature_extractor.post_process(outputs, target_sizes)\n",
    "    bboxes_scaled = postprocessed_outputs[0]['boxes'][keep]\n",
    "    return (probas[keep], bboxes_scaled)\n",
    "\n",
    "def plot_results_detection(model, pil_img, prob, boxes, delta_xmin, delta_ymin, delta_xmax, delta_ymax):\n",
    "    # crop_tables and plot_results_detection must have same co-ord shifts because 1 only plots the other one updates co-ordinates\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    for p, (xmin, ymin, xmax, ymax) in zip(prob, boxes.tolist()):\n",
    "        cl = p.argmax()\n",
    "        xmin, ymin, xmax, ymax = xmin - delta_xmin, ymin - delta_ymin, xmax + delta_xmax, ymax + delta_ymax\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red', linewidth=3))\n",
    "        text = f'{model.config.id2label[cl.item()]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin - 20, ymin - 50, text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "def crop_tables(pil_img, prob, boxes, delta_xmin, delta_ymin, delta_xmax, delta_ymax):\n",
    "    # crop_tables and plot_results_detection must have same co-ord shifts because 1 only plots the other one updates co-ordinates\n",
    "    cropped_img_list = []\n",
    "    for _, (xmin, ymin, xmax, ymax) in zip(prob, boxes.tolist()):\n",
    "        xmin, ymin, xmax, ymax = xmin - delta_xmin, ymin - delta_ymin, xmax + delta_xmax, ymax + delta_ymax\n",
    "        cropped_img = pil_img.crop((xmin, ymin, xmax, ymax))\n",
    "        cropped_img_list.append(cropped_img)\n",
    "    return cropped_img_list\n",
    "\n",
    "def add_padding(pil_img, top, right, bottom, left, color=(255, 255, 255)):\n",
    "    # Image padding as part of TSR pre-processing to prevent missing table edges\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"violet\"]\n",
    "def generate_structure(model, pil_img, prob, boxes, expand_rowcol_bbox_top, expand_rowcol_bbox_bottom):\n",
    "    # plt.figure(figsize=(32, 20))\n",
    "    # plt.imshow(pil_img)\n",
    "    # ax = plt.gca()\n",
    "    rows = {}\n",
    "    cols = {}\n",
    "    idx = 0\n",
    "    for p, (xmin, ymin, xmax, ymax) in zip(prob, boxes.tolist()):\n",
    "        xmin, ymin, xmax, ymax = xmin, ymin, xmax, ymax\n",
    "        cl = p.argmax()\n",
    "        class_text = model.config.id2label[cl.item()]\n",
    "        text = f'{class_text}: {p[cl]:0.2f}'\n",
    "        # or (class_text == 'table column')\n",
    "        # if (class_text == 'table row') or (class_text == 'table projected row header') or (class_text == 'table column'):\n",
    "        #     ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=colors[cl.item()], linewidth=2))\n",
    "        #     ax.text(xmin - 10, ymin - 10, text, fontsize=5, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "        if class_text == 'table row':\n",
    "            rows['table row.' + str(idx)] = (xmin, ymin - expand_rowcol_bbox_top, xmax, ymax + expand_rowcol_bbox_bottom)\n",
    "        if class_text == 'table column':\n",
    "            cols['table column.' + str(idx)] = (xmin, ymin - expand_rowcol_bbox_top, xmax, ymax + expand_rowcol_bbox_bottom)\n",
    "        idx += 1\n",
    "    return rows, cols\n",
    "\n",
    "def sort_table_featuresv2(rows: dict, cols: dict):\n",
    "    # Sometimes the header and first row overlap, and we need the header bbox not to have first row's bbox inside the headers bbox\n",
    "    rows_ = {\n",
    "        table_feature: (xmin, ymin, xmax, ymax)\n",
    "        for table_feature, (xmin, ymin, xmax, ymax) in sorted(rows.items(), key=lambda tup: tup[1][1])\n",
    "    }\n",
    "    cols_ = {\n",
    "        table_feature: (xmin, ymin, xmax, ymax)\n",
    "        for table_feature, (xmin, ymin, xmax, ymax) in sorted(cols.items(), key=lambda tup: tup[1][0])\n",
    "    }\n",
    "    return rows_, cols_\n",
    "\n",
    "def individual_table_featuresv2(pil_img, rows: dict, cols: dict):\n",
    "    for k, v in rows.items():\n",
    "        xmin, ymin, xmax, ymax = v\n",
    "        cropped_img = pil_img.crop((xmin, ymin, xmax, ymax))\n",
    "        rows[k] = xmin, ymin, xmax, ymax, cropped_img\n",
    "    for k, v in cols.items():\n",
    "        xmin, ymin, xmax, ymax = v\n",
    "        cropped_img = pil_img.crop((xmin, ymin, xmax, ymax))\n",
    "        cols[k] = xmin, ymin, xmax, ymax, cropped_img\n",
    "    return rows, cols\n",
    "\n",
    "def object_to_cellsv2(master_row: dict, cols: dict, padd_left):\n",
    "    cells_img = {}\n",
    "    row_idx = 0\n",
    "    new_cols = {}\n",
    "    new_master_row = {}\n",
    "    new_cols = cols\n",
    "    new_master_row = master_row\n",
    "    for k_row, v_row in new_master_row.items():\n",
    "        _, _, _, _, row_img = v_row\n",
    "        xmax, ymax = row_img.size\n",
    "        xa, ya, xb, yb = 0, 0, 0, ymax\n",
    "        row_img_list = []\n",
    "        for idx, kv in enumerate(new_cols.items()):\n",
    "            k_col, v_col = kv\n",
    "            xmin_col, _, xmax_col, _, col_img = v_col\n",
    "            xmin_col, xmax_col = xmin_col - padd_left - 10, xmax_col - padd_left\n",
    "            xa = xmin_col\n",
    "            xb = xmax_col\n",
    "            if idx == 0:\n",
    "                xa = 0\n",
    "            if idx == len(new_cols) - 1:\n",
    "                xb = xmax\n",
    "            xa, ya, xb, yb = xa, ya, xb, yb\n",
    "            row_img_cropped = row_img.crop((xa, ya, xb, yb))\n",
    "            row_img_list.append(row_img_cropped)\n",
    "        cells_img[k_row + '.' + str(row_idx)] = row_img_list\n",
    "        row_idx += 1\n",
    "    return cells_img, len(new_cols), len(new_master_row) - 1\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].str.replace(\"'\", '', regex=True)\n",
    "        df[col] = df[col].str.replace('\"', '', regex=True)\n",
    "        df[col] = df[col].str.replace(']', '', regex=True)\n",
    "        df[col] = df[col].str.replace('[', '', regex=True)\n",
    "        df[col] = df[col].str.replace('{', '', regex=True)\n",
    "        df[col] = df[col].str.replace('}', '', regex=True)\n",
    "    return df\n",
    "\n",
    "def convert_df(df):\n",
    "    return df.to_csv().encode('utf-8')\n",
    "\n",
    "def create_dataframe(cell_ocr_res: list, max_cols: int, max_rows: int):\n",
    "    headers = cell_ocr_res[:max_cols]\n",
    "    new_headers = uniquify(headers, (f' {x!s}' for x in string.ascii_lowercase))\n",
    "    counter = 0\n",
    "    cells_list = cell_ocr_res[max_cols:]\n",
    "    df = pd.DataFrame(\"\", index=range(0, max_rows), columns=new_headers)\n",
    "    cell_idx = 0\n",
    "    for nrows in range(max_rows):\n",
    "        for ncols in range(max_cols):\n",
    "            df.iat[nrows, ncols] = str(cells_list[cell_idx])\n",
    "            cell_idx += 1\n",
    "    for x, col in zip(string.ascii_lowercase, new_headers):\n",
    "        if f' {x!s}' == col:\n",
    "            counter += 1\n",
    "    df = clean_dataframe(df)\n",
    "    return df\n",
    "\n",
    "def start_process(img, padd_top, padd_left, padd_bottom, padd_right, delta_xmin, delta_ymin, delta_xmax,\n",
    "                  delta_ymax, expand_rowcol_bbox_top, expand_rowcol_bbox_bottom):\n",
    "    image=Image.fromarray(img).convert(\"RGB\")\n",
    "    probas, bboxes_scaled = table_detector(image)\n",
    "    if bboxes_scaled.nelement() == 0:\n",
    "        return ''\n",
    "    # plot_results_detection(table_detection_model, image, probas, bboxes_scaled, delta_xmin, delta_ymin, delta_xmax, delta_ymax)\n",
    "    cropped_img_list = crop_tables(image, probas, bboxes_scaled, delta_xmin, delta_ymin, delta_xmax, delta_ymax)\n",
    "    for unpadded_table in cropped_img_list:\n",
    "        table = add_padding(unpadded_table, padd_top, padd_right, padd_bottom, padd_left)\n",
    "        probas, bboxes_scaled = table_struct_recog(table)\n",
    "        rows, cols = generate_structure(table_recognition_model, table, probas, bboxes_scaled, expand_rowcol_bbox_top, expand_rowcol_bbox_bottom)\n",
    "        rows, cols = sort_table_featuresv2(rows, cols)\n",
    "        master_row, cols = individual_table_featuresv2(table, rows, cols)\n",
    "        cells_img, max_cols, max_rows = object_to_cellsv2(master_row, cols, padd_left)\n",
    "        sequential_cell_img_list = []\n",
    "        for k, img_list in cells_img.items():\n",
    "            for img in img_list:\n",
    "                sequential_cell_img_list.append(\n",
    "                    pytess(cell_pil_img=img))\n",
    "        cell_ocr_res = sequential_cell_img_list\n",
    "        df=create_dataframe(cell_ocr_res, max_cols, max_rows)\n",
    "        return df\n",
    "\n",
    "def get_tables_from_pdf(path):\n",
    "    os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk/libexec/openjdk.jdk\"\n",
    "    tables = read_pdf(path, stream=True, multiple_tables=True, pages='all', encoding='utf-8')\n",
    "    return tables\n",
    "\n",
    "def get_images_from_pdf(path):\n",
    "    images = []\n",
    "    pdf_file = fitz.open(path)\n",
    "    for page_index in range(len(pdf_file)):\n",
    "        page = pdf_file[page_index] \n",
    "        for _, img in enumerate(page.get_images(), start=1): \n",
    "            # get the XREF of the image \n",
    "            xref = img[0] \n",
    "            # extract the image bytes \n",
    "            base_image = pdf_file.extract_image(xref) \n",
    "            image_bytes = base_image[\"image\"]\n",
    "            nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "            imageFile = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            images.append(imageFile)\n",
    "    return images\n",
    "\n",
    "def image_to_table(img):\n",
    "    return start_process(img, 20, 20, 20, 20, 0, 0, 0, 0, 0, 0)\n",
    "\n",
    "def extract_table_data_from_pdf(file_path):\n",
    "    images = get_images_from_pdf(file_path)\n",
    "    df = []\n",
    "    for image in images:\n",
    "        df.append(image_to_table(image))\n",
    "    return df\n",
    "\n",
    "def extract_all_tables(file_path):\n",
    "    data1 = extract_table_data_from_pdf(file_path)\n",
    "    data2 = get_tables_from_pdf(file_path)\n",
    "    for d in data1:\n",
    "        data2.append(d)\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Table QA bot\n",
    "def process_data_for_qa_bot(d):\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            process_data_for_qa_bot(v)\n",
    "        else:\n",
    "            if type(v) == int:\n",
    "                v = str(v)\n",
    "            d.update({k: v})\n",
    "    return d\n",
    "\n",
    "def qa_bot_on_table(table, queries):\n",
    "    data = pipe(process_data_for_qa_bot(table.to_dict()), queries)\n",
    "    if type(data) == type([]):\n",
    "        answers = []\n",
    "        for l in data:\n",
    "            answers.append(l['answer'])\n",
    "        return answers\n",
    "    else:\n",
    "        return data['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(path_to_pdf):\n",
    "    tables = extract_all_tables(path_to_pdf)\n",
    "    print(\"Welcome to Tabvision.AI. Ask what you want to.\")\n",
    "    while(True):\n",
    "        flag1 = 0\n",
    "        i = 1\n",
    "        for table in tables:\n",
    "            print(\"Table: \" + str(i))\n",
    "            print(table)\n",
    "            print(table.to_dict())\n",
    "            i = i + 1\n",
    "            flag2 = 0\n",
    "            while(True):\n",
    "                query = [input(\"Enter your query or Enter 'next' for next table or Enter 'exit' to finish: \")]\n",
    "                if query[0]=='next':\n",
    "                    break\n",
    "                elif query[0]=='exit':\n",
    "                    flag2 = 1\n",
    "                    break\n",
    "                else:\n",
    "                    print(qa_bot_on_table(table, query))\n",
    "            if flag2 == 1:\n",
    "                flag1 = 1\n",
    "                break\n",
    "        if flag1 == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./samples/Sample.pdf\"\n",
    "chatbot(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
